{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738df6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e179e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyByCatDM(LightningDataModule):\n",
    "    def __init__(self, setupdir, train_frac=0.9, seed=0, batch_size=64):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.setupdir = setupdir\n",
    "        self.train_frac = train_frac\n",
    "        self.seed = seed\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "              transforms.Resize(size=256),\n",
    "              transforms.CenterCrop(size=224),\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                   [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def setup(self):\n",
    "        \n",
    "        torch.manual_seed(self.seed)\n",
    "        \n",
    "        dataset = datasets.ImageFolder(self.setupdir)\n",
    "        self.num_classes = len(dataset.classes)\n",
    "        \n",
    "        set_len = len(dataset)\n",
    "        train_len = int(set_len * self.train_frac)\n",
    "        val_len = int(set_len * (1 - self.train_frac) / 2)\n",
    "        test_len = set_len - train_len - val_len\n",
    "        \n",
    "        self.train, self.val, self.test = random_split(dataset, \n",
    "                                                      [train_len,\n",
    "                                                       val_len,\n",
    "                                                       test_len])\n",
    "        self.train.dataset.transform = self.transform\n",
    "        \n",
    "        self.val.dataset.transform = self.transform\n",
    "        \n",
    "        self.test.dataset.transform = self.transform\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d46437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyModel(LightningModule):\n",
    "    def __init__(self, input_shape, num_classes,\n",
    "                 learning_rate = 1e-4, batch_size=64):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dim = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.feature_extractor = models.resnet34(pretrained=True)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        n_sizes = self._get_conv_output(input_shape)\n",
    "        self.classifier = torch.nn.Linear(n_sizes, num_classes)\n",
    "        \n",
    "        self.predictions = []\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        \n",
    "        batch_size = 1\n",
    "        inp = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        \n",
    "        features = self._forward_features(inp)\n",
    "        n_size = features.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "    \n",
    "    def _forward_features(self, x):\n",
    "        \n",
    "        x = self.feature_extractor(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = functional.log_softmax(self.classifier(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = functional.nll_loss(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = functional.nll_loss(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = functional.nll_loss(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            self.predictions.append(preds[i])\n",
    "\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc43859",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dm = ClassifyByCatDM(setupdir='small_dataset_sorted_by_cat', train_frac=0.5,\n",
    "                  seed=0, batch_size=batch_size)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af27241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "num_classes = dm.num_classes\n",
    "model = ClassifyModel((3,224,224), num_classes,\n",
    "                      batch_size=batch_size, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b050f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/homebrew/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name              | Type   | Params\n",
      "---------------------------------------------\n",
      "0 | feature_extractor | ResNet | 21.8 M\n",
      "1 | classifier        | Linear | 5.0 K \n",
      "---------------------------------------------\n",
      "21.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.8 M    Total params\n",
      "87.211    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.functional.classification.accuracy.accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|██████████▋     | 6/9 [00:01<00:00,  4.48it/s, loss=2.55, v_num=5]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9/9 [00:01<00:00,  5.49it/s, loss=2.55, v_num=5, val_loss=2.490\u001b[A\n",
      "Epoch 1:  67%|▋| 6/9 [00:01<00:00,  4.63it/s, loss=2.24, v_num=5, val_loss=2.490\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 9/9 [00:01<00:00,  5.66it/s, loss=2.24, v_num=5, val_loss=2.280\u001b[A\n",
      "Epoch 2:  67%|▋| 6/9 [00:01<00:00,  4.51it/s, loss=1.98, v_num=5, val_loss=2.280\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 9/9 [00:01<00:00,  5.46it/s, loss=1.98, v_num=5, val_loss=1.680\u001b[A\n",
      "Epoch 3:  67%|▋| 6/9 [00:01<00:00,  4.43it/s, loss=1.5, v_num=5, val_loss=1.680,\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 9/9 [00:01<00:00,  5.36it/s, loss=1.5, v_num=5, val_loss=0.973,\u001b[A\n",
      "Epoch 3: 100%|█| 9/9 [00:02<00:00,  4.60it/s, loss=1.5, v_num=5, val_loss=0.973,\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "/opt/homebrew/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing:  75%|███████████████████████████         | 3/4 [00:00<00:00, 11.59it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.2857142984867096, 'test_loss': 2.3450584411621094}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|████████████████████████████████████| 4/4 [00:00<00:00, 12.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.3450584411621094, 'test_acc': 0.2857142984867096}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=4,\n",
    "                  progress_bar_refresh_rate=1)\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62168f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
